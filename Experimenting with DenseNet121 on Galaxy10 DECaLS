{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12989198,"sourceType":"datasetVersion","datasetId":8221663}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nfrom pathlib import Path\nfrom collections import Counter\nimport h5py\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import models, transforms\nfrom torchinfo import summary\nfrom torch.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T21:36:21.665256Z","iopub.execute_input":"2025-10-06T21:36:21.665537Z","iopub.status.idle":"2025-10-06T21:36:21.670923Z","shell.execute_reply.started":"2025-10-06T21:36:21.665517Z","shell.execute_reply":"2025-10-06T21:36:21.670120Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ------------------ Config ------------------\nclass CFG:\n    data_path = \"/kaggle/input/galaxy10-decals/Galaxy10_DECals.h5\"\n    batch_size = 64\n    num_workers = 4\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_classes = 10\n    num_epochs = 20\n    seed = 42\n    out_dir = \"/kaggle/working/galaxy10_experiments\"\n    plot_dir = \"/kaggle/working/galaxy10_plots\"\n    experiments_summary_csv = os.path.join(out_dir, \"experiments_summary.csv\")\n    experiments_with_balanced_csv = os.path.join(out_dir, \"experiments_with_balanced.csv\")\n    experiments_with_imbalanced_csv = os.path.join(out_dir, \"experiments_with_imbalanced.csv\")\n    use_data_parallel = True  # Use multi-GPU if available\n    run = 0\n    \nos.makedirs(CFG.plot_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T21:36:24.821773Z","iopub.execute_input":"2025-10-06T21:36:24.822544Z","iopub.status.idle":"2025-10-06T21:36:24.914710Z","shell.execute_reply.started":"2025-10-06T21:36:24.822505Z","shell.execute_reply":"2025-10-06T21:36:24.913990Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ------------------ Reproducibility ------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T21:38:42.000290Z","iopub.execute_input":"2025-10-06T21:38:42.000578Z","iopub.status.idle":"2025-10-06T21:38:42.010559Z","shell.execute_reply.started":"2025-10-06T21:38:42.000557Z","shell.execute_reply":"2025-10-06T21:38:42.009826Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ------------------ Dataset ------------------\nclass GalaxyH5Dataset(Dataset):\n    def __init__(self, images_array, labels_array, indices, transform=None):\n        self.images = images_array\n        self.labels = labels_array\n        self.indices = np.array(indices, dtype=np.int64)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        real_idx = self.indices[idx]\n        img = self.images[real_idx]  # HWC, uint8\n        # Convert to PIL Image\n        if img.dtype != np.uint8:\n            img = (img * 255).astype(np.uint8)  # guard if floats in [0,1]\n        pil = Image.fromarray(img)\n        label = int(self.labels[real_idx])\n        if self.transform:\n            pil = self.transform(pil)\n        return pil, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T21:38:45.332206Z","iopub.execute_input":"2025-10-06T21:38:45.332515Z","iopub.status.idle":"2025-10-06T21:38:45.337878Z","shell.execute_reply.started":"2025-10-06T21:38:45.332494Z","shell.execute_reply":"2025-10-06T21:38:45.337125Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ------------------ Transforms ------------------\ndef get_transforms():\n    # ImageNet normalization\n    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n    IMAGENET_STD  = [0.229, 0.224, 0.225]\n    \n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),  # Resize to 224x224\n        transforms.RandomApply([transforms.RandomRotation(30)], p=0.33),  # Random rotation with p=0.33\n        transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)], p=0.33),  # Random contrast/brightness with p=0.33\n        transforms.RandomHorizontalFlip(p=0.33),  # Random horizontal flip with p=0.33\n        transforms.RandomVerticalFlip(p=0.33),  # Random vertical flip with p=0.33\n        transforms.ToTensor(),\n        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n    ])\n    return train_transform, val_transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T21:38:53.615589Z","iopub.execute_input":"2025-10-06T21:38:53.616144Z","iopub.status.idle":"2025-10-06T21:38:53.623304Z","shell.execute_reply.started":"2025-10-06T21:38:53.616121Z","shell.execute_reply":"2025-10-06T21:38:53.622555Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ------------------ Model Factory ------------------\ndef build_model(model_name=\"densenet121\", num_classes=10, pretrained=True):\n    if model_name == \"densenet121\":\n        weights = models.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None\n        model = models.densenet121(weights=weights)\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n    # elif model_name == \"resnet50\":\n    #     model = models.resnet50(pretrained=True)\n    #     model.fc = nn.Linear(model.fc.in_features, num_classes)\n    else:\n        raise ValueError(\"Unsupported model\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:03:55.769657Z","iopub.execute_input":"2025-09-21T17:03:55.770140Z","iopub.status.idle":"2025-09-21T17:03:55.774593Z","shell.execute_reply.started":"2025-09-21T17:03:55.770114Z","shell.execute_reply":"2025-09-21T17:03:55.773721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ Training & Validation ------------------\ndef train_one_epoch(model, loader, optimizer, criterion, device, scaler):\n    model.train()\n    running_loss=0.0\n    preds = []\n    targs = []\n    n = 0\n\n    for images, labels in loader:\n        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n        optimizer.zero_grad()\n        with autocast(device_type=\"cuda\"):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        running_loss += loss.item() * images.size(0)\n        n += images.size(0)\n        \n        preds_batch = outputs.argmax(dim=1).detach().cpu().numpy()\n        preds.extend(preds_batch.tolist())\n        targs.extend(labels.detach().cpu().numpy().tolist())\n\n    epoch_loss = running_loss / n\n    epoch_acc = accuracy_score(targs, preds)\n    \n    return epoch_loss, epoch_acc\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    preds = []\n    targs = []\n    n = 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            with autocast(device_type = \"cuda\"):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n            n += images.size(0)\n            preds.extend(outputs.argmax(dim=1).detach().cpu().numpy().tolist())\n            targs.extend(labels.detach().cpu().numpy().tolist())\n    epoch_loss = running_loss / n\n    epoch_acc = accuracy_score(targs, preds)\n\n    return epoch_loss, epoch_acc, np.array(targs), np.array(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:03:56.931317Z","iopub.execute_input":"2025-09-21T17:03:56.931805Z","iopub.status.idle":"2025-09-21T17:03:56.940275Z","shell.execute_reply.started":"2025-09-21T17:03:56.931781Z","shell.execute_reply":"2025-09-21T17:03:56.939438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_dataset(images, labels):\n    CLASS_NAMES = [\n    \"Disturbed Galaxies\",\n    \"Merging Galaxies\",\n    \"Round Smooth Galaxies\",\n    \"In-between Round Smooth Galaxies\",\n    \"Cigar Shaped Smooth Galaxies\",\n    \"Barred Spiral Galaxies\",\n    \"Unbarred Tight Spiral Galaxies\",\n    \"Unbarred Loose Spiral Galaxies\",\n    \"Edge-on Galaxies without Bulge\",\n    \"Edge-on Galaxies with Bulge\"\n]\n    sample_per_class = {}\n    # Loop through dataset to find one image per class\n    for img, label in zip(images, labels):\n        if label not in sample_per_class:\n            sample_per_class[label] = img\n        if len(sample_per_class) == 10:\n            break\n    \n    # Plot one image per class\n    plt.figure(figsize=(18, 6))\n    plt.xticks([])\n    plt.yticks([])\n    for i in range(10):\n        plt.subplot(2, 5, i + 1)\n        plt.imshow(sample_per_class[i])\n        plt.title(f\"{CLASS_NAMES[i]}\")\n        plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(CFG.plot_dir,\"Dataset Classes.png\"))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T21:39:18.120211Z","iopub.execute_input":"2025-10-06T21:39:18.120807Z","iopub.status.idle":"2025-10-06T21:39:18.126408Z","shell.execute_reply.started":"2025-10-06T21:39:18.120777Z","shell.execute_reply":"2025-10-06T21:39:18.125524Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"images_ds, labels_ds, df = read_dataset(CFG.data_path)\n\ndisplay_dataset(images_ds, labels_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load `.h5` (images and labels) and show dataset summary\n# This reads the file into RAM (17k images — should fit on a Kaggle session comfortably).\ndef read_dataset(h5_path):\n    with h5py.File(h5_path, \"r\") as f:\n        # Inspect keys\n        #print(\"HDF5 keys:\", list(f.keys()))\n        # Common structure for Galaxy10_DECals: 'images' and 'ans'\n        # but sometimes keys differ; attempt robust extraction\n        if \"images\" in f:\n            images_ds = f[\"images\"][:]\n        elif \"imgs\" in f:\n            images_ds = f[\"imgs\"][:]\n        else:\n            # look for first dataset with shape (N, H, W, C)\n            found = None\n            for k in f.keys():\n                ds = f[k]\n                if isinstance(ds, h5py.Dataset) and ds.ndim == 4:\n                    found = k\n                    break\n            if found is None:\n                raise RuntimeError(\"Could not find images dataset inside the HDF5 file.\")\n            images_ds = f[found][:]\n            print(\"Using images dataset key:\", found)\n    \n        # labels\n        if \"ans\" in f:\n            labels_ds = f[\"ans\"][:]\n        elif \"labels\" in f:\n            labels_ds = f[\"labels\"][:]\n        else:\n            # fallback: find 1D int dataset\n            found_label = None\n            for k in f.keys():\n                ds = f[k]\n                if isinstance(ds, h5py.Dataset) and ds.ndim == 1:\n                    found_label = k\n                    break\n            if found_label is None:\n                raise RuntimeError(\"Could not find label dataset inside the HDF5 file.\")\n            labels_ds = f[found_label][:]\n            print(\"Using labels dataset key:\", found_label)\n    \n    #print(\"Images shape:\", images_ds.shape)\n    #print(\"Labels shape:\", labels_ds.shape)\n    assert images_ds.shape[0] == labels_ds.shape[0], \"Image count and label count mismatch\"\n    N = images_ds.shape[0]\n    #print(\"Total samples:\", N)\n    class_counts = Counter(labels_ds.tolist())\n    #print(\"Class distribution (label:count):\", dict(class_counts))\n    num_classes = len(sorted(set(labels_ds.tolist())))\n    #print(\"Detected num classes:\", num_classes)\n\n    #Build a DataFrame index for stratified splitting (lightweight)\n    df = pd.DataFrame({\"idx\": np.arange(N), \"label\": labels_ds})\n    \n    return images_ds, labels_ds, df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:03:59.576090Z","iopub.execute_input":"2025-09-21T17:03:59.576757Z","iopub.status.idle":"2025-09-21T17:03:59.583958Z","shell.execute_reply.started":"2025-09-21T17:03:59.576732Z","shell.execute_reply":"2025-09-21T17:03:59.583181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_history(exp, history):\n    epochs = history[\"epoch\"]\n\n    # Plot Loss\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training vs Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Accuracy\n    plt.subplot(1, 3, 2)\n    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Training vs Validation Accuracy\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Learning Rate\n    plt.subplot(1, 3, 3)\n    plt.plot(epochs, history[\"lr\"], label=\"Learning Rate\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"LR\")\n    plt.title(\"Learning Rate Schedule\")\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n\n    plt.savefig(os.path.join(CFG.plot_dir,f\"{exp}_training_history.png\"))\n    \n    plt.show()    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:04:04.582560Z","iopub.execute_input":"2025-09-21T17:04:04.582826Z","iopub.status.idle":"2025-09-21T17:04:04.589270Z","shell.execute_reply.started":"2025-09-21T17:04:04.582808Z","shell.execute_reply":"2025-09-21T17:04:04.588542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(exp, y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred, labels=np.arange(10))\n\n    # 3. Plot the Confusion Matrix\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title(f\"{exp}\")\n    \n    # 4. Save the Plot\n    plt.savefig(os.path.join(CFG.plot_dir,f\"{exp}_confusion_matrix.png\"))\n    \n    # 5. Show the plot (optional)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:04:05.058183Z","iopub.execute_input":"2025-09-21T17:04:05.058385Z","iopub.status.idle":"2025-09-21T17:04:05.062545Z","shell.execute_reply.started":"2025-09-21T17:04:05.058371Z","shell.execute_reply":"2025-09-21T17:04:05.061866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Focal Loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0):\n        super().__init__()\n        self.alpha = torch.tensor(alpha, dtype=torch.float32) if alpha is not None else None\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        if self.alpha is not None:\n            at = self.alpha.to(inputs.device)[targets]\n        else:\n            at = 1.0\n            \n        return (at * (1-pt)**self.gamma * ce_loss).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:04:05.413665Z","iopub.execute_input":"2025-09-21T17:04:05.414150Z","iopub.status.idle":"2025-09-21T17:04:05.418919Z","shell.execute_reply.started":"2025-09-21T17:04:05.414132Z","shell.execute_reply":"2025-09-21T17:04:05.418260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ Run Experiment ------------------\ndef run_experiment(exp_name, dataset_balanced, lr, optimizer_name,  num_of_epochs):\n    print(f\"\\n==================================================\")\n    print(f\"Running Experiment: {exp_name}\")\n    print(f\"Model: DenseNet121, Optimizer: {optimizer_name}, LR: {lr}, Scheduler: CosineAnnealingLR, Loss: Focal Loss, Dataset Balanced: {dataset_balanced}\")\n    print(f\"==================================================\\n\")\n\n    CFG.num_epochs = num_of_epochs\n    CFG.run += 1\n    # Prepare data\n    images_ds, labels_ds, df = read_dataset(CFG.data_path)\n\n    # Display dataset on first run\n    if CFG.run == 1:\n        display_dataset(images_ds, labels_ds)\n\n    if dataset_balanced == True:\n        # Set target per-class sample size = min class count (334 for class 4)\n        min_count = df[\"label\"].value_counts().min()\n        balanced_indices = []\n        # Randomly sample exactly 334 indices per class\n        rng = np.random.default_rng(CFG.seed)\n        for cls, group in df.groupby(\"label\"):\n            sampled = rng.choice(group[\"idx\"].values, size=min_count, replace=False)\n            balanced_indices.extend(sampled)\n        balanced_df = df[df[\"idx\"].isin(balanced_indices)].reset_index(drop=True)\n        df = balanced_df\n\n    # Stratified split: train 75%, val 15%, test 10%\n    train_idx, temp_idx, y_train, y_temp = train_test_split(\n        df[\"idx\"].values, df[\"label\"].values, stratify=df[\"label\"].values,\n        test_size=0.2, random_state=CFG.seed\n    )\n    val_idx, test_idx, y_val, y_test = train_test_split(\n        temp_idx, y_temp, stratify=y_temp,\n        test_size=0.5, random_state=CFG.seed\n    )\n    \n    print(\"\\nTrain:\", len(train_idx), \"Val:\", len(val_idx), \"Test:\", len(test_idx))\n    print(\"Train class counts:\", Counter(labels_ds[train_idx].tolist()))\n    print(\"Val class counts:\", Counter(labels_ds[val_idx].tolist()))\n    print(\"Test class counts:\", Counter(labels_ds[test_idx].tolist()))\n    print(\"\")\n\n    train_tf, val_tf = get_transforms()\n    train_ds = GalaxyH5Dataset(images_ds, labels_ds, train_idx, transform=train_tf)\n    val_ds = GalaxyH5Dataset(images_ds, labels_ds, val_idx, transform=val_tf)\n    test_ds = GalaxyH5Dataset(images_ds, labels_ds, test_idx, transform=val_tf)\n\n    train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n    \n    # Model\n    model = build_model(\"densenet121\", CFG.num_classes, pretrained=True)\n\n    model = model.to(CFG.device)\n    \n    if CFG.device.type == \"cuda\" and CFG.use_data_parallel and torch.cuda.device_count() > 1:\n        if CFG.run == 1:\n            print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n        model = nn.DataParallel(model)\n    \n    # Optimizers\n    if optimizer_name == \"AdamW\":\n        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    elif optimizer_name == \"SGDM\":\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n    else:\n        raise ValueError(\"Unsupported optimizer\")\n\n    #if scheduler_name == \"reduce_on_plateau\":\n    #    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',  # 'min' means we monitor the loss or metric being minimized\n    #                                                     factor=0.1,  # Reduces LR by a factor of 0.1 when the metric stops improving\n    #                                                     patience=0,  # Number of epochs with no improvement before LR is reduced\n    #                                                     min_lr=1e-6)\n    \n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_of_epochs, eta_min=1e-6)\n\n    if dataset_balanced == True:\n        criterion = FocalLoss(alpha=None, gamma=2)\n    else:\n        counts = np.array([1081, 1853, 2645, 2027, 334, 2043, 1829, 2628, 1423, 1873])\n        beta = 0.9999\n        effective_num = 1.0 - np.power(beta, counts)\n        wts = (1.0 - beta) / effective_num\n        wts = wts / wts.sum() * 10    # Normalize to 10 classes\n        \n        criterion = FocalLoss(alpha=wts, gamma=2)\n\n    # Cross Entropy Loss\n    #criterion = nn.CrossEntropyLoss()\n    \n    # Training Loop\n    history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], 'lr': [] }\n    total_time_taken = 0.0\n    best_val_acc = 0\n    for epoch in range(1, CFG.num_epochs+1):\n        start_time = time.time()\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, CFG.device, GradScaler(\"cuda\"))\n        val_loss, val_acc, val_targs, val_preds = validate(model, val_loader, criterion, CFG.device)\n        elapsed = time.time() - start_time\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        current_lr = optimizer.param_groups[0]['lr']\n        history[\"lr\"].append(current_lr)\n        \n        #scheduler.step(float(val_loss))  # works with ReduceLROnPlateau Scheduler\n        \n        scheduler.step()   # works with CosineAnnealingLR Scheduler\n\n        total_time_taken += elapsed\n\n        print(f\"Epoch [{epoch}/{CFG.num_epochs}] Completed | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | Learning Rate: {current_lr:.8f} | Time: {elapsed:.2f}s\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_wts = model.state_dict().copy()\n    \n    plot_training_history(exp_name, history)\n    \n    model.load_state_dict(best_model_wts)\n\n    # Test\n    test_start_time = time.time()\n    \n    test_loss, test_acc, test_targs, test_preds = validate(model, test_loader, criterion, CFG.device)\n    \n    overall_testing_time = time.time() - test_start_time\n    testing_time_per_image = overall_testing_time / len(test_preds)\n    \n    print(f\"\\nTest Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Time: {overall_testing_time:.2f}s\")\n    precision, recall, f1, _ = precision_recall_fscore_support(test_targs, test_preds, average=None, labels=np.arange(10))\n    overall_prec, overall_rec, overall_f1, _ = precision_recall_fscore_support(test_targs, test_preds, average='weighted')\n    \n    print(\"\\nOverall (weighted) Precision: {:.4f} Recall: {:.4f} F1: {:.4f} \\n\".format(overall_prec, overall_rec, overall_f1))\n    \n    # Build a DataFrame for per-class metrics\n    per_class_df = pd.DataFrame({\n        \"class\": np.arange(10),\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"support\": np.bincount(test_targs, minlength=10)\n    })\n    print(per_class_df)\n    report = classification_report(test_targs, test_preds, output_dict=True)\n\n    plot_confusion_matrix(exp_name, test_targs, test_preds)\n\n    os.makedirs(CFG.out_dir, exist_ok=True)\n    exp_out = os.path.join(CFG.out_dir, f\"{exp_name}_report.csv\")\n    pd.DataFrame(report).transpose().to_csv(exp_out)\n    return {\"exp_name\":exp_name, \"Data\":dataset_balanced, \"Learning Rate\":lr, \"Optimizer\":optimizer_name, \"Epochs\": num_of_epochs, \"Model Training Time\":f\"{total_time_taken:.2f}s\", \"Testing Time (per image)\":f\"{testing_time_per_image:.2f}s\", \"Overall Accuracy\": test_acc*100, \"Class 0\":recall[0] * 100, \"Class 1\":recall[1] * 100, \"Class 2\":recall[2] * 100, \"Class 3\":recall[3] * 100, \"Class 4\":recall[4] * 100, \"Class 5\":recall[5] * 100, \"Class 6\":recall[6] * 100, \"Class 7\":recall[7] * 100, \"Class 8\":recall[8] * 100, \"Class 9\":recall[9] * 100}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:04:08.293077Z","iopub.execute_input":"2025-09-21T17:04:08.293342Z","iopub.status.idle":"2025-09-21T17:04:08.313731Z","shell.execute_reply.started":"2025-09-21T17:04:08.293324Z","shell.execute_reply":"2025-09-21T17:04:08.313015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ Experiments on Balanced Data ------------------\nEXPERIMENTS = [\n    {\"exp_name\":\"AdamW on Balanced Data LR 1e-3 - 20 epochs\", \"dataset_balanced\":True, \"lr\":1e-3, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"AdamW on Balanced Data LR 1e-4 - 20 epochs\", \"dataset_balanced\":True, \"lr\":1e-4, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"SGDM on Balanced Data LR 1e-3 - 20 epochs\", \"dataset_balanced\":True, \"lr\":1e-3, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"SGDM on Balanced Data LR 1e-4 - 20 epochs\", \"dataset_balanced\":True, \"lr\":1e-4, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"AdamW on Balanced Data LR 1e-3 - 40 epochs\", \"dataset_balanced\":True, \"lr\":1e-3, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":40},\n    {\"exp_name\":\"AdamW on Balanced Data LR 1e-4 - 40 epochs\", \"dataset_balanced\":True, \"lr\":1e-4, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":40},\n    {\"exp_name\":\"SGDM on Balanced Data LR 1e-3 - 40 epochs\", \"dataset_balanced\":True, \"lr\":1e-3, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":40},\n    {\"exp_name\":\"SGDM on Balanced Data LR 1e-4 - 40 epochs\", \"dataset_balanced\":True, \"lr\":1e-4, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":40},\n    ]\n\nbalanced_results = []\nfor exp in EXPERIMENTS:\n    res = run_experiment(**exp)\n    balanced_results.append(res)\n\npd.DataFrame(balanced_results).to_csv(CFG.experiments_with_balanced_csv, index=False)\nprint(\"\\nExperiments with Balanced Data completed. Summary saved.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T17:04:08.533934Z","iopub.execute_input":"2025-09-21T17:04:08.534174Z","iopub.status.idle":"2025-09-21T17:06:32.440592Z","shell.execute_reply.started":"2025-09-21T17:04:08.534157Z","shell.execute_reply":"2025-09-21T17:06:32.439458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ Experiments on Imbalanced Data ------------------\nEXPERIMENTS = [\n    {\"exp_name\":\"AdamW on Imbalanced Data LR 1e-3 - 20 epochs\", \"dataset_balanced\":False, \"lr\":1e-3, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"AdamW on Imbalanced Data LR 1e-4 - 20 epochs\", \"dataset_balanced\":False, \"lr\":1e-4, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"SGDM on Imbalanced Data LR 1e-3 - 20 epochs\", \"dataset_balanced\":False, \"lr\":1e-3, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"SGDM on Imbalanced Data LR 1e-4 - 20 epochs\", \"dataset_balanced\":False, \"lr\":1e-4, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":20},\n    {\"exp_name\":\"AdamW on Imbalanced Data LR 1e-3 - 40 epochs\", \"dataset_balanced\":False, \"lr\":1e-3, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":40},\n    {\"exp_name\":\"AdamW on Imbalanced Data LR 1e-4 - 40 epochs\", \"dataset_balanced\":False, \"lr\":1e-4, \"optimizer_name\":\"AdamW\",  \"num_of_epochs\":40},\n    {\"exp_name\":\"SGDM on Imbalanced Data LR 1e-3 - 40 epochs\", \"dataset_balanced\":False, \"lr\":1e-3, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":40},\n    {\"exp_name\":\"SGDM on Imbalanced Data LR 1e-4 - 40 epochs\", \"dataset_balanced\":False, \"lr\":1e-4, \"optimizer_name\":\"SGDM\",  \"num_of_epochs\":40}\n    ]\n\nresults = balanced_results\nimbalanced_results = []\nfor exp in EXPERIMENTS:\n    res = run_experiment(**exp)\n    imbalanced_results.append(res)\n    results.append(res)\n\npd.DataFrame(imbalanced_results).to_csv(CFG.experiments_with_imbalanced_csv, index=False)\nprint(\"\\nExperiments with Imbalanced Data completed. Summary saved.\")\n\npd.DataFrame(results).to_csv(CFG.experiments_summary_csv, index=False)\nprint(\"\\nAll experiments completed. Summary saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:17:28.318259Z","iopub.execute_input":"2025-09-21T16:17:28.318535Z","iopub.status.idle":"2025-09-21T17:02:03.624061Z","shell.execute_reply.started":"2025-09-21T16:17:28.318515Z","shell.execute_reply":"2025-09-21T17:02:03.622794Z"}},"outputs":[],"execution_count":null}]}